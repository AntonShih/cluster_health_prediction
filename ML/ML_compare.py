# ====================================================================
# ç³–å°¿ç—…é æ¸¬æ©Ÿå™¨å­¸ç¿’ç³»çµ± - ç²¾ç°¡ç‰ˆ
# æ•´åˆï¼šé è™•ç† + ç‰¹å¾µå·¥ç¨‹ + æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ + è©•ä¼°
# ====================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
import xgboost as xgb
import lightgbm as lgb
import warnings
warnings.filterwarnings('ignore')

# ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ====================================================================
# ğŸ¯ ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆï¼Ÿ
# ====================================================================
"""
é‡å°ä½ çš„ç³–å°¿ç—…è³‡æ–™é›†(100Kç­†, 16ç‰¹å¾µ)çš„è¨­è¨ˆè€ƒé‡ï¼š

1. é†«å­¸ç‰¹å¾µå·¥ç¨‹ï¼š
   - HbA1c >= 6.5, è¡€ç³– >= 126 â†’ è¨ºæ–·æ¨™æº–ç‰¹å¾µ
   - BMIåˆ†ç´šã€å¹´é½¡åˆ†çµ„ â†’ é¢¨éšªåˆ†å±¤
   - ç¨®æ—é¢¨éšªã€äº¤äº’ä½œç”¨ â†’ é†«å­¸çŸ¥è­˜

2. é è™•ç†ç­–ç•¥ï¼š
   - æ•¸å€¼ç‰¹å¾µ â†’ StandardScaler (é©åˆLR)
   - é¡åˆ¥ç‰¹å¾µ â†’ OneHotEncoder (é¿å…é †åºéŒ¯èª¤)
   - ColumnTransformer â†’ é˜²æ­¢è³‡æ–™æ´©æ¼

3. æ¨¡å‹é¸æ“‡ï¼š
   - LogisticRegression â†’ å¯è§£é‡‹æ€§(é†«ç™‚é‡è¦)
   - RandomForest â†’ è™•ç†äº¤äº’ä½œç”¨
   - XGBoost/LightGBM â†’ éç·šæ€§å»ºæ¨¡
   - Ensemble â†’ æå‡ç©©å®šæ€§
"""

def load_and_create_features(filepath, target_col='diabetes'):
    """è¼‰å…¥è³‡æ–™ä¸¦å‰µå»ºé†«å­¸ç‰¹å¾µ"""
    print("ğŸš€ è¼‰å…¥è³‡æ–™ä¸¦é€²è¡Œé†«å­¸ç‰¹å¾µå·¥ç¨‹...")
    
    df = pd.read_csv(filepath)
    print(f"åŸå§‹è³‡æ–™ï¼š{df.shape[0]:,} ç­†, {df.shape[1]} æ¬„ä½")
    print(f"ç³–å°¿ç—…æ‚£ç—…ç‡ï¼š{df[target_col].mean():.1%}")
    
    # é†«å­¸ç‰¹å¾µå·¥ç¨‹
    if 'hbA1c_level' in df.columns:
        df['HbA1c_Diabetic'] = (df['hbA1c_level'] >= 6.5).astype(int)
        df['HbA1c_Prediabetic'] = ((df['hbA1c_level'] >= 5.7) & (df['hbA1c_level'] < 6.5)).astype(int)
    
    if 'blood_glucose_level' in df.columns:
        df['Glucose_Diabetic'] = (df['blood_glucose_level'] >= 126).astype(int)
        df['Glucose_Prediabetic'] = ((df['blood_glucose_level'] >= 100) & (df['blood_glucose_level'] < 126)).astype(int)
    
    if 'bmi' in df.columns:
        df['BMI_Obese'] = (df['bmi'] >= 30).astype(int)
        df['BMI_Overweight'] = ((df['bmi'] >= 25) & (df['bmi'] < 30)).astype(int)
        df['BMI_Squared'] = df['bmi'] ** 2
    
    if 'age' in df.columns:
        df['Age_High_Risk'] = (df['age'] >= 45).astype(int)
        df['Age_Squared'] = df['age'] ** 2
    
    if 'hypertension' in df.columns and 'heart_disease' in df.columns:
        df['CVD_Risk'] = df['hypertension'] + df['heart_disease']
    
    # å¸è¸é¢¨éšªé‡åŒ–
    if 'smoking_history' in df.columns:
        smoking_map = {'never': 0, 'No Info': 0.2, 'former': 0.6, 'current': 1.0, 'not current': 0.3, 'ever': 0.8}
        df['Smoking_Risk'] = df['smoking_history'].map(lambda x: smoking_map.get(x, 0.2))
    
    # é«˜é¢¨éšªç¨®æ—
    if 'race:AfricanAmerican' in df.columns or 'race:Hispanic' in df.columns:
        df['High_Risk_Race'] = 0
        if 'race:AfricanAmerican' in df.columns:
            df['High_Risk_Race'] |= df['race:AfricanAmerican']
        if 'race:Hispanic' in df.columns:
            df['High_Risk_Race'] |= df['race:Hispanic']
    
    # äº¤äº’ä½œç”¨ç‰¹å¾µ
    if 'age' in df.columns and 'bmi' in df.columns:
        df['Age_BMI_Interaction'] = df['age'] * df['bmi'] / 1000
    
    print(f"ç‰¹å¾µå·¥ç¨‹å¾Œï¼š{df.shape[1]} æ¬„ä½")
    return df

def preprocess_data(df, target_col='diabetes'):
    """æ™ºèƒ½é è™•ç†ç®¡é“"""
    print("ğŸ¤– åŸ·è¡Œæ™ºèƒ½é è™•ç†...")
    
    X = df.drop(columns=[target_col])
    y = df[target_col]
    
    # è­˜åˆ¥ç‰¹å¾µé¡å‹
    numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
    
    print(f"æ•¸å€¼ç‰¹å¾µï¼š{len(numerical_cols)} å€‹")
    print(f"é¡åˆ¥ç‰¹å¾µï¼š{len(categorical_cols)} å€‹ {categorical_cols}")
    
    # å»ºç«‹é è™•ç†ç®¡é“
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numerical_cols),
            ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols)
        ]
    )
    
    X_processed = preprocessor.fit_transform(X)
    
    # ç²å–ç‰¹å¾µåç¨±
    feature_names = numerical_cols.copy()
    if categorical_cols:
        cat_encoder = preprocessor.named_transformers_['cat']
        for i, col in enumerate(categorical_cols):
            categories = cat_encoder.categories_[i][1:]  # drop='first'
            feature_names.extend([f"{col}_{cat}" for cat in categories])
    
    X_df = pd.DataFrame(X_processed, columns=feature_names)
    print(f"é è™•ç†å®Œæˆï¼š{X_df.shape[1]} ç‰¹å¾µ")
    
    return X_df, y, preprocessor

def select_features(X, y, min_features=15, save_plots=True, output_dir="diabetes_ml_results"):
    """RFECVç‰¹å¾µé¸æ“‡ä¸¦å„²å­˜è¦–è¦ºåŒ–"""
    print("ğŸ¯ åŸ·è¡ŒRFECVç‰¹å¾µé¸æ“‡...")
    
    # åªç”¨80%è³‡æ–™åšç‰¹å¾µé¸æ“‡ï¼Œé¿å…éæ“¬åˆ
    X_fs, _, y_fs, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    selector = RFECV(
        estimator=LogisticRegression(max_iter=1000, random_state=42),
        step=1,
        cv=StratifiedKFold(5, shuffle=True, random_state=42),
        scoring='roc_auc',
        min_features_to_select=min_features,
        n_jobs=-1
    )
    
    selector.fit(X_fs, y_fs)
    
    selected_features = X.columns[selector.support_].tolist()
    X_selected = X[selected_features]
    
    print(f"é¸å‡ºç‰¹å¾µæ•¸ï¼š{len(selected_features)}")
    print(f"æœ€ä½³CVåˆ†æ•¸ï¼š{max(selector.cv_results_['mean_test_score']):.4f}")
    
    # è¦–è¦ºåŒ–RFECVçµæœ
    if save_plots:
        import os
        os.makedirs(f"{output_dir}/plots", exist_ok=True)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # RFECVåˆ†æ•¸æ›²ç·š
    scores = selector.cv_results_['mean_test_score']
    ax1.plot(range(1, len(scores) + 1), scores, 'bo-', linewidth=2)
    ax1.axvline(x=selector.n_features_, color='red', linestyle='--', 
               label=f'æœ€ä½³ç‰¹å¾µæ•¸: {selector.n_features_}')
    ax1.set_xlabel('ç‰¹å¾µæ•¸é‡')
    ax1.set_ylabel('äº¤å‰é©—è­‰ AUC åˆ†æ•¸')
    ax1.set_title('RFECV ç‰¹å¾µé¸æ“‡çµæœ')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Top 10 é‡è¦ç‰¹å¾µ
    importance = np.abs(selector.estimator_.coef_[0])
    top_10_idx = np.argsort(importance)[-10:]
    
    ax2.barh(range(10), importance[top_10_idx])
    ax2.set_yticks(range(10))
    ax2.set_yticklabels([selected_features[i] for i in top_10_idx])
    ax2.set_xlabel('ç‰¹å¾µé‡è¦æ€§ (çµ•å°ä¿‚æ•¸å€¼)')
    ax2.set_title('Top 10 é‡è¦ç‰¹å¾µ')
    
    plt.tight_layout()
    if save_plots:
        plt.savefig(f"{output_dir}/plots/feature_selection.png", dpi=300, bbox_inches='tight')
        print(f"ğŸ¯ ç‰¹å¾µé¸æ“‡åœ–å·²å„²å­˜ï¼š{output_dir}/plots/feature_selection.png")
    plt.show()
    
    return X_selected, selected_features, selector

def train_models(X, y, test_size=0.2):
    """è¨“ç·´å¤šå€‹æ©Ÿå™¨å­¸ç¿’æ¨¡å‹"""
    print("ğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹...")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=y
    )
    
    models = {}
    metrics = {}
    
    # 1. Logistic Regression
    print("ğŸ“Š è¨“ç·´ Logistic Regression...")
    lr = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
    lr.fit(X_train, y_train)
    models['LogisticRegression'] = lr
    metrics['LogisticRegression'] = evaluate_model(lr, X_test, y_test)
    
    # 2. Random Forest
    print("ğŸŒ² è¨“ç·´ Random Forest...")
    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)
    rf.fit(X_train, y_train)
    models['RandomForest'] = rf
    metrics['RandomForest'] = evaluate_model(rf, X_test, y_test)
    
    # 3. XGBoost
    print("âš¡ è¨“ç·´ XGBoost...")
    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
    xgb_model = xgb.XGBClassifier(
        n_estimators=100, max_depth=6, learning_rate=0.1,
        scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss'
    )
    xgb_model.fit(X_train, y_train)
    models['XGBoost'] = xgb_model
    metrics['XGBoost'] = evaluate_model(xgb_model, X_test, y_test)
    
    # 4. LightGBM
    print("ğŸ’¡ è¨“ç·´ LightGBM...")
    lgb_model = lgb.LGBMClassifier(
        n_estimators=100, max_depth=6, learning_rate=0.1,
        class_weight='balanced', random_state=42, verbose=-1
    )
    lgb_model.fit(X_train, y_train)
    models['LightGBM'] = lgb_model
    metrics['LightGBM'] = evaluate_model(lgb_model, X_test, y_test)
    
    # 5. Ensemble (Top 3) - ä¿®å¾©ç‰ˆæœ¬ç›¸å®¹æ€§å•é¡Œ
    print("ğŸ­ è¨“ç·´ Ensemble...")
    try:
        model_scores = [(name, m['auc']) for name, m in metrics.items()]
        model_scores.sort(key=lambda x: x[1], reverse=True)
        top_3 = model_scores[:3]
        
        # é‡æ–°è¨“ç·´é¸ä¸­çš„æ¨¡å‹ï¼Œç¢ºä¿ç›¸å®¹æ€§
        ensemble_models = []
        for name, _ in top_3:
            if name == 'LogisticRegression':
                model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
            elif name == 'RandomForest':
                model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)
            elif name == 'XGBoost':
                model = xgb.XGBClassifier(
                    n_estimators=100, max_depth=6, learning_rate=0.1,
                    scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss'
                )
            elif name == 'LightGBM':
                model = lgb.LGBMClassifier(
                    n_estimators=100, max_depth=6, learning_rate=0.1,
                    class_weight='balanced', random_state=42, verbose=-1
                )
            
            model.fit(X_train, y_train)
            ensemble_models.append((name, model))
        
        weights = [score for _, score in top_3]
        
        ensemble = VotingClassifier(
            estimators=ensemble_models,
            voting='soft',
            weights=weights
        )
        ensemble.fit(X_train, y_train)
        models['Ensemble'] = ensemble
        metrics['Ensemble'] = evaluate_model(ensemble, X_test, y_test)
        
        print(f"ğŸ† Ensemble ä½¿ç”¨ï¼š{[name for name, _ in top_3]}")
        
    except Exception as e:
        print(f"âš ï¸  Ensemble è¨“ç·´å¤±æ•—: {str(e)}")
        print("ğŸ“Š ç¹¼çºŒä½¿ç”¨å…¶ä»–4å€‹æ¨¡å‹...")
        
        # å¦‚æœ Ensemble å¤±æ•—ï¼Œå»ºç«‹ç°¡å–®çš„å¹³å‡é æ¸¬
        print("ğŸ”„ å»ºç«‹ç°¡å–®å¹³å‡ Ensemble...")
        try:
            # ç°¡å–®å¹³å‡é æ¸¬
            all_probas = []
            for name, model in models.items():
                y_proba = model.predict_proba(X_test)[:, 1]
                all_probas.append(y_proba)
            
            avg_proba = np.mean(all_probas, axis=0)
            avg_pred = (avg_proba > 0.5).astype(int)
            
            # å»ºç«‹è™›æ“¬ ensemble è©•ä¼°
            ensemble_metrics = {
                'accuracy': accuracy_score(y_test, avg_pred),
                'precision': precision_score(y_test, avg_pred),
                'recall': recall_score(y_test, avg_pred),
                'f1': f1_score(y_test, avg_pred),
                'auc': roc_auc_score(y_test, avg_proba)
            }
            
            metrics['SimpleEnsemble'] = ensemble_metrics
            print("âœ… ç°¡å–®å¹³å‡ Ensemble å®Œæˆ")
            
        except Exception as e2:
            print(f"âŒ ç°¡å–® Ensemble ä¹Ÿå¤±æ•—: {str(e2)}")
            print("ğŸ“Š åªä½¿ç”¨å–®ä¸€æ¨¡å‹çµæœ")
    
    return models, metrics, (X_train, X_test, y_train, y_test)

def evaluate_model(model, X_test, y_test):
    """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    
    return {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'auc': roc_auc_score(y_test, y_proba)
    }

def plot_results(metrics, models=None, X_test=None, y_test=None, save_plots=True, output_dir="diabetes_ml_results"):
    """ç¹ªè£½çµæœæ¯”è¼ƒä¸¦å„²å­˜åœ–ç‰‡"""
    # æ€§èƒ½æ¯”è¼ƒè¡¨
    results_df = pd.DataFrame(metrics).T.round(4)
    print("\nğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¼ƒï¼š")
    print(results_df)
    
    best_model = results_df['auc'].idxmax()
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹ï¼š{best_model} (AUC: {results_df.loc[best_model, 'auc']:.4f})")
    
    if save_plots:
        import os
        os.makedirs(f"{output_dir}/plots", exist_ok=True)
    
    # 1. ç¹ªè£½AUCæ¯”è¼ƒåœ–å’ŒROCæ›²ç·š
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    auc_scores = results_df['auc'].sort_values()
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']
    bars = plt.barh(range(len(auc_scores)), auc_scores.values, color=colors[:len(auc_scores)])
    plt.yticks(range(len(auc_scores)), auc_scores.index)
    plt.xlabel('AUC Score')
    plt.title('ğŸ¯ æ¨¡å‹ AUC æ¯”è¼ƒ')
    
    # åœ¨æ¢å½¢åœ–ä¸Šé¡¯ç¤ºæ•¸å€¼
    for i, (bar, value) in enumerate(zip(bars, auc_scores.values)):
        plt.text(value + 0.005, bar.get_y() + bar.get_height()/2, 
                f'{value:.3f}', va='center', fontsize=10)
    
    # å¦‚æœæœ‰æ¨¡å‹å’Œæ¸¬è©¦è³‡æ–™ï¼Œç¹ªè£½æœ€ä½³æ¨¡å‹çš„ROCæ›²ç·š
    if models and X_test is not None and y_test is not None:
        plt.subplot(1, 2, 2)
        best_model_obj = models[best_model]
        y_proba = best_model_obj.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_proba)
        
        plt.plot(fpr, tpr, linewidth=2, label=f'{best_model} (AUC = {results_df.loc[best_model, "auc"]:.3f})')
        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.8)
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ğŸ“ˆ {best_model} ROC æ›²ç·š')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    if save_plots:
        plt.savefig(f"{output_dir}/plots/model_comparison.png", dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š æ¨¡å‹æ¯”è¼ƒåœ–å·²å„²å­˜ï¼š{output_dir}/plots/model_comparison.png")
    plt.show()
    
    # 2. ç¹ªè£½æ‰€æœ‰æ¨¡å‹çš„ROCæ›²ç·šå°æ¯”
    if models and X_test is not None and y_test is not None:
        plt.figure(figsize=(8, 6))
        
        for i, (name, model) in enumerate(models.items()):
            y_proba = model.predict_proba(X_test)[:, 1]
            fpr, tpr, _ = roc_curve(y_test, y_proba)
            auc_score = results_df.loc[name, 'auc']
            
            plt.plot(fpr, tpr, linewidth=2, color=colors[i % len(colors)],
                    label=f'{name} (AUC = {auc_score:.3f})')
        
        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.8)
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ğŸ“ˆ æ‰€æœ‰æ¨¡å‹ ROC æ›²ç·šæ¯”è¼ƒ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        if save_plots:
            plt.savefig(f"{output_dir}/plots/all_roc_curves.png", dpi=300, bbox_inches='tight')
            print(f"ğŸ“ˆ ROCæ›²ç·šæ¯”è¼ƒåœ–å·²å„²å­˜ï¼š{output_dir}/plots/all_roc_curves.png")
        plt.show()
    
    # 3. ç¹ªè£½æ··æ·†çŸ©é™£ï¼ˆæ‰€æœ‰æ¨¡å‹ï¼‰
    if models and X_test is not None and y_test is not None:
        n_models = len(models)
        cols = 3 if n_models > 3 else n_models
        rows = (n_models + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))
        if n_models == 1:
            axes = [axes]
        elif rows == 1:
            axes = axes.reshape(1, -1)
        
        for i, (name, model) in enumerate(models.items()):
            row, col = i // cols, i % cols
            ax = axes[row, col] if rows > 1 else axes[col]
            
            y_pred = model.predict(X_test)
            cm = confusion_matrix(y_test, y_pred)
            
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                       xticklabels=['éç³–å°¿ç—…', 'ç³–å°¿ç—…'],
                       yticklabels=['éç³–å°¿ç—…', 'ç³–å°¿ç—…'])
            ax.set_title(f'ğŸ” {name} æ··æ·†çŸ©é™£')
            ax.set_xlabel('é æ¸¬å€¼')
            ax.set_ylabel('çœŸå¯¦å€¼')
        
        # éš±è—å¤šé¤˜çš„å­åœ–
        for i in range(n_models, rows * cols):
            row, col = i // cols, i % cols
            axes[row, col].set_visible(False)
        
        plt.tight_layout()
        if save_plots:
            plt.savefig(f"{output_dir}/plots/confusion_matrices.png", dpi=300, bbox_inches='tight')
            print(f"ğŸ” æ··æ·†çŸ©é™£åœ–å·²å„²å­˜ï¼š{output_dir}/plots/confusion_matrices.png")
        plt.show()
    
    # 4. ç¹ªè£½é›·é”åœ–
    metrics_list = ['accuracy', 'precision', 'recall', 'f1', 'auc']
    angles = np.linspace(0, 2 * np.pi, len(metrics_list), endpoint=False).tolist()
    angles += angles[:1]
    
    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
    
    for i, (model_name, model_metrics) in enumerate(results_df.iterrows()):
        values = [model_metrics[metric] for metric in metrics_list]
        values += values[:1]
        
        ax.plot(angles, values, 'o-', linewidth=2, 
                label=model_name, color=colors[i % len(colors)])
        ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels([m.upper() for m in metrics_list])
    ax.set_ylim(0, 1)
    ax.set_title('ğŸ¯ æ¨¡å‹æ€§èƒ½é›·é”åœ–', size=16, fontweight='bold', pad=20)
    ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
    
    plt.tight_layout()
    if save_plots:
        plt.savefig(f"{output_dir}/plots/performance_radar.png", dpi=300, bbox_inches='tight')
        print(f"ğŸ¯ é›·é”åœ–å·²å„²å­˜ï¼š{output_dir}/plots/performance_radar.png")
    plt.show()
    
    return results_df

def save_results(models, preprocessor, selector, selected_features, metrics, output_dir="diabetes_ml_results"):
    """å„²å­˜æ‰€æœ‰çµæœåŒ…å«åœ–ç‰‡"""
    import os
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(f"{output_dir}/plots", exist_ok=True)
    
    # å„²å­˜æ¨¡å‹
    for name, model in models.items():
        joblib.dump(model, f"{output_dir}/{name}_model.pkl")
    
    # å„²å­˜é è™•ç†å™¨å’Œç‰¹å¾µé¸æ“‡å™¨
    joblib.dump(preprocessor, f"{output_dir}/preprocessor.pkl")
    joblib.dump(selector, f"{output_dir}/feature_selector.pkl")
    
    # å„²å­˜ç‰¹å¾µåç¨±
    with open(f"{output_dir}/selected_features.txt", 'w', encoding='utf-8') as f:
        f.write('\n'.join(selected_features))
    
    # å„²å­˜æ€§èƒ½æŒ‡æ¨™ç‚ºCSV
    results_df = pd.DataFrame(metrics).T.round(4)
    results_df.to_csv(f"{output_dir}/model_performance.csv", encoding='utf-8')
    
    # å»ºç«‹çµæœæ‘˜è¦å ±å‘Š
    best_model = results_df['auc'].idxmax()
    best_auc = results_df['auc'].max()
    
    report = f"""
# ç³–å°¿ç—…é æ¸¬æ¨¡å‹è¨“ç·´çµæœæ‘˜è¦

## ğŸ† æœ€ä½³æ¨¡å‹
- æ¨¡å‹åç¨±ï¼š{best_model}
- AUC åˆ†æ•¸ï¼š{best_auc:.4f}
- æº–ç¢ºç‡ï¼š{results_df.loc[best_model, 'accuracy']:.4f}
- ç²¾ç¢ºç‡ï¼š{results_df.loc[best_model, 'precision']:.4f}
- å¬å›ç‡ï¼š{results_df.loc[best_model, 'recall']:.4f}
- F1åˆ†æ•¸ï¼š{results_df.loc[best_model, 'f1']:.4f}

## ğŸ“Š æ‰€æœ‰æ¨¡å‹æ€§èƒ½
{results_df.to_string()}

## ğŸ¯ é¸ä¸­çš„é—œéµç‰¹å¾µ (å…±{len(selected_features)}å€‹)
{chr(10).join([f"- {feature}" for feature in selected_features[:15]])}
{'...' if len(selected_features) > 15 else ''}

## ğŸ“ æª”æ¡ˆèªªæ˜
- æ¨¡å‹æª”æ¡ˆï¼š*_model.pkl
- é è™•ç†å™¨ï¼špreprocessor.pkl
- ç‰¹å¾µé¸æ“‡å™¨ï¼šfeature_selector.pkl
- æ€§èƒ½æ¯”è¼ƒåœ–ï¼šplots/model_comparison.png
- ROCæ›²ç·šæ¯”è¼ƒï¼šplots/all_roc_curves.png
- é›·é”åœ–ï¼šplots/performance_radar.png
- ç‰¹å¾µé¸æ“‡åœ–ï¼šplots/feature_selection.png
"""
    
    with open(f"{output_dir}/README.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… æ‰€æœ‰çµæœå·²å„²å­˜è‡³ï¼š{output_dir}/")
    print(f"ğŸ“Š åŒ…å« 5 å¼µåœ–ç‰‡ï¼š")
    print(f"   - {output_dir}/plots/model_comparison.png")
    print(f"   - {output_dir}/plots/all_roc_curves.png")
    print(f"   - {output_dir}/plots/confusion_matrices.png")
    print(f"   - {output_dir}/plots/performance_radar.png")
    print(f"   - {output_dir}/plots/feature_selection.png")
    print(f"ğŸ“„ æ‘˜è¦å ±å‘Šï¼š{output_dir}/README.md")

# ====================================================================
# ä¸»è¦åŸ·è¡Œå‡½æ•¸
# ====================================================================

def main(filepath="data/diabetes_dataset.csv", target_col='diabetes'):
    """å®Œæ•´åŸ·è¡Œæµç¨‹"""
    print("ğŸ¯ ç³–å°¿ç—…é æ¸¬æ©Ÿå™¨å­¸ç¿’ç³»çµ±")
    print("=" * 50)
    
    # 1. è¼‰å…¥è³‡æ–™ä¸¦ç‰¹å¾µå·¥ç¨‹
    df = load_and_create_features(filepath, target_col)
    
    # 2. é è™•ç†
    X_processed, y, preprocessor = preprocess_data(df, target_col)
    
    # 3. ç‰¹å¾µé¸æ“‡
    X_selected, selected_features, selector = select_features(X_processed, y)
    
    # 4. è¨“ç·´æ¨¡å‹
    models, metrics, (X_train, X_test, y_train, y_test) = train_models(X_selected, y)
    
    # 5. çµæœæ¯”è¼ƒï¼ˆåŒ…å«åœ–ç‰‡å„²å­˜ï¼‰
    results_df = plot_results(metrics, models, X_test, y_test, save_plots=True, output_dir="diabetes_ml_results")
    
    # 6. å„²å­˜æ‰€æœ‰çµæœï¼ˆåŒ…å«åœ–ç‰‡ï¼‰
    save_results(models, preprocessor, selector, selected_features, metrics, "diabetes_ml_results")
    
    print("\nğŸ‰ å®Œæˆï¼")
    return models, metrics, results_df, selected_features

# åŸ·è¡Œç¯„ä¾‹
if __name__ == "__main__":
    # åŸ·è¡Œå®Œæ•´æµç¨‹
    models, metrics, results_df, selected_features = main()
    
    print(f"\nğŸ¯ é‡é»ç‰¹å¾µï¼š{selected_features[:10]}")
    print(f"ğŸ“Š æœ€çµ‚çµæœï¼š")
    print(results_df)